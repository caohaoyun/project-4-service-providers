{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "status             object\n",
       "bed               float64\n",
       "bath              float64\n",
       "acre_lot          float64\n",
       "city               object\n",
       "state              object\n",
       "zip_code          float64\n",
       "house_size        float64\n",
       "prev_sold_date     object\n",
       "price             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "allproperties = pd.read_csv('Data/original_data/realtor-data.zip.csv')\n",
    "allproperties.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'sold_previously' column based on 'prev_sold_date'\n",
    "allproperties['sold_previously'] = allproperties['prev_sold_date'].notna().astype(int)\n",
    "# Drop the 'prev_sold_date' column\n",
    "allproperties = allproperties.drop(columns=['prev_sold_date', 'status'])\n",
    "# Fill missing values in 'bed' and 'bath' columns with 1\n",
    "allproperties['bed'].fillna(1, inplace=True)\n",
    "allproperties['bath'].fillna(1, inplace=True)\n",
    "# Drop rows with NaNs in the 'zip_code' column\n",
    "allproperties.dropna(subset=['zip_code', 'price'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nan_count = allproperties['house_size'].isna().sum()\n",
    "\n",
    "#print(f\"Number of NaNs in 'house_size' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new data directory\n",
    "output_dir = 'Data/new_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Minimum number of rows required (15,000 in this case)\n",
    "min_rows = 15000\n",
    "\n",
    "# Loop through unique states and create separate CSV files\n",
    "unique_states = allproperties['state'].unique()\n",
    "for state in unique_states:\n",
    "    state_df = allproperties[allproperties['state'] == state]\n",
    "\n",
    "    # Check the number of rows in the DataFrame\n",
    "    num_rows = len(state_df)\n",
    "\n",
    "    # Only save the file if it has over 15,000 rows\n",
    "    if num_rows >= min_rows:\n",
    "        output_file = os.path.join(output_dir, f'{state}_properties.csv')\n",
    "        state_df.to_csv(output_file, index=False)\n",
    "    else:\n",
    "        print(f\"Skipped {state}_properties.csv with {num_rows} rows (less than 15,000).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate unique IDs\n",
    "def generate_ids(df):\n",
    "    state = df['state'].iloc[0][:3]  # Get the first 3 letters of the state\n",
    "    df['ID'] = state + df.groupby('state').cumcount().add(1).astype(str)\n",
    "    return df.set_index('ID')  # Set the 'ID' column as the index\n",
    "\n",
    "# Directory containing the newly created CSV files\n",
    "input_dir = 'Data/new_data'\n",
    "\n",
    "# Loop through the CSV files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Add the ID column and set it as the index\n",
    "        df = generate_ids(df)\n",
    "        \n",
    "        # Save the DataFrame back to the CSV file with the ID as the index\n",
    "        df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file to store the NaN counts\n",
    "output_file = 'nan_counts.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # Loop through the CSV files\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            filepath = os.path.join(input_dir, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            # Count NaNs for each column in the DataFrame\n",
    "            nan_counts = df.isna().sum()\n",
    "\n",
    "            # Get the total number of rows\n",
    "            total_rows = len(df)\n",
    "\n",
    "            # Write the NaN counts and total rows for each file to the text file\n",
    "            f.write(f\"NaN counts and total rows for {filename}:\\n\")\n",
    "            for column in df.columns:\n",
    "                f.write(f\"{column}: {nan_counts[column]}\\n\")\n",
    "            f.write(f\"Total Rows: {total_rows}\\n\")\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
