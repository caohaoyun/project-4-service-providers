{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from geopy.geocoders import Nominatim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "allproperties = pd.read_csv('Data/original_data/realtor-data.zip.csv')\n",
    "#allproperties.dtypes\n",
    "#new data directory\n",
    "output_dir = 'Data/new_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Directory containing the newly created CSV files\n",
    "input_dir = 'Data/new_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the 'sold_previously' column based on 'prev_sold_date'\n",
    "allproperties['sold_previously'] = allproperties['prev_sold_date'].notna().astype(int)\n",
    "# Drop the 'prev_sold_date' column\n",
    "allproperties = allproperties.drop(columns=['prev_sold_date', 'status'])\n",
    "# Fill missing values in 'bed' and 'bath' columns with 1\n",
    "allproperties['bed'].fillna(1, inplace=True)\n",
    "allproperties['bath'].fillna(1, inplace=True)\n",
    "# Drop rows with NaNs in the 'zip_code' column\n",
    "allproperties.dropna(subset=['zip_code', 'price', 'city'], inplace=True)\n",
    "# Drop rows with 'price' less than 2000\n",
    "allproperties = allproperties[allproperties['price'] >= 1000]\n",
    "# Drop rows with 'house_size' less than or equal to 0\n",
    "allproperties = allproperties[allproperties['house_size'] > 0]\n",
    "# Drop rows with 'acre_lot' less than or equal to 0\n",
    "allproperties = allproperties[allproperties['acre_lot'] > 0]\n",
    "# Multiply negative values in 'acre_lot' column by -1 to make them positive\n",
    "allproperties['acre_lot'] = allproperties['acre_lot'].apply(lambda x: x * -1 if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove outliers from specific columns\n",
    "\n",
    "# Calculate the IQR for each column\n",
    "Q1 = allproperties[['price', 'bed', 'house_size', 'acre_lot', 'bath']].quantile(0.25)\n",
    "Q3 = allproperties[['price', 'bed', 'house_size', 'acre_lot', 'bath']].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the upper bound for outliers\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Remove outliers\n",
    "filtered_df = allproperties[\n",
    "    (allproperties['price'] <= upper_bound['price']) &\n",
    "    (allproperties['bed'] <= upper_bound['bed']) &\n",
    "    (allproperties['house_size'] <= upper_bound['house_size']) &\n",
    "    (allproperties['acre_lot'] <= upper_bound['acre_lot']) &\n",
    "    (allproperties['bath'] <= upper_bound['bath'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped Virgin Islands_properties.csv with 345 rows (less than 15,000).\n",
      "Skipped Wyoming_properties.csv with 3 rows (less than 15,000).\n",
      "Skipped West Virginia_properties.csv with 3 rows (less than 15,000).\n",
      "Skipped Delaware_properties.csv with 1742 rows (less than 15,000).\n"
     ]
    }
   ],
   "source": [
    "# Minimum number of rows required (15,000 in this case)\n",
    "min_rows = 13000\n",
    "\n",
    "# Loop through unique states and create separate CSV files\n",
    "unique_states = allproperties['state'].unique()\n",
    "for state in unique_states:\n",
    "    state_df = allproperties[allproperties['state'] == state]\n",
    "\n",
    "    # Check the number of rows in the DataFrame\n",
    "    num_rows = len(state_df)\n",
    "\n",
    "    # Only save the file if it has over 15,000 rows\n",
    "    if num_rows >= min_rows:\n",
    "        output_file = os.path.join(output_dir, f'{state}_properties.csv')\n",
    "        state_df.to_csv(output_file, index=False)\n",
    "    else:\n",
    "        print(f\"Skipped {state}_properties.csv with {num_rows} rows (less than 15,000).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424426, 9)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allproperties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to generate unique IDs\n",
    "def generate_ids(df):\n",
    "    state = df['state'].iloc[0][:5].replace(' ', '')  # Get the first 4 letters of the state\n",
    "    df['ID'] = state + df.groupby('state').cumcount().add(1).astype(str)\n",
    "    return df.set_index('ID')  # Set the 'ID' column as the index\n",
    "\n",
    "# Loop through the CSV files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Add the ID column and set it as the index\n",
    "        df = generate_ids(df)\n",
    "        \n",
    "        # Save the DataFrame back to the CSV file with the ID as the index\n",
    "        df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the CSV files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Check the number of rows in the DataFrame\n",
    "        num_rows = len(df)\n",
    "\n",
    "        # Only fill NaN values for files with over 15,000 rows\n",
    "        if num_rows >= min_rows:\n",
    "            # Filter data with non-NaN 'house_size'\n",
    "            valid_data = df.dropna(subset=['house_size'])\n",
    "\n",
    "            # Separate features (bed, bath) and target (house_size)\n",
    "            X = valid_data[['bed', 'bath']]\n",
    "            y = valid_data['house_size']\n",
    "\n",
    "            # Train a linear regression model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "\n",
    "            # Predict 'house_size' for rows with NaNs\n",
    "            nan_data = df[pd.isna(df['house_size'])]\n",
    "            if not nan_data.empty:\n",
    "                nan_X = nan_data[['bed', 'bath']]\n",
    "                nan_data['house_size'] = model.predict(nan_X)\n",
    "\n",
    "                # Round the predicted 'house_size' values to the nearest whole number\n",
    "                nan_data['house_size'] = nan_data['house_size'].round()\n",
    "\n",
    "                # Update the original DataFrame with filled 'house_size'\n",
    "                df.update(nan_data)\n",
    "\n",
    "                # Save the updated DataFrame back to the same CSV file\n",
    "                df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the CSV files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "        # Check the number of rows in the DataFrame\n",
    "        num_rows = len(df)\n",
    "\n",
    "        # Only fill NaN values for files with over 15,000 rows\n",
    "        if num_rows >= min_rows:\n",
    "            # Filter data with non-NaN 'acre_lot'\n",
    "            valid_data = df.dropna(subset=['acre_lot'])\n",
    "\n",
    "            # Separate features ('house_size', 'bed', 'bath') and target ('acre_lot')\n",
    "            X = valid_data[['house_size', 'bed', 'bath']]\n",
    "            y = valid_data['acre_lot']\n",
    "\n",
    "            # Train a linear regression model\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "\n",
    "            # Predict 'acre_lot' for rows with NaNs\n",
    "            nan_data = df[pd.isna(df['acre_lot'])]\n",
    "            if not nan_data.empty:\n",
    "                nan_X = nan_data[['house_size', 'bed', 'bath']]\n",
    "                nan_data['acre_lot'] = model.predict(nan_X)\n",
    "\n",
    "                # Round the predicted 'acre_lot' values to one decimal place\n",
    "                nan_data['acre_lot'] = nan_data['acre_lot'].round(1)\n",
    "\n",
    "                # Update the original DataFrame with filled 'acre_lot'\n",
    "                df.update(nan_data)\n",
    "\n",
    "                # Save the updated DataFrame back to the same CSV file\n",
    "                df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the \"uszips.csv\" file into a DataFrame\n",
    "uszips_df = pd.read_csv('Data/original_data/uszips.csv')\n",
    "\n",
    "# Function to get latitude and longitude for a zip code\n",
    "def get_lat_long(zip_code):\n",
    "    try:\n",
    "        # Find the corresponding row in uszips_df based on the zip code\n",
    "        row = uszips_df[uszips_df['zip'] == int(zip_code)]\n",
    "        if not row.empty:\n",
    "            return row.iloc[0]['lat'], row.iloc[0]['lng']\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return None, None\n",
    "\n",
    "# Loop through the CSV files\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Read the CSV file into a DataFrame\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        \n",
    "        # Apply the get_lat_long function to each row to get latitude and longitude\n",
    "        df['latitude'], df['longitude'] = zip(*df['zip_code'].map(get_lat_long))\n",
    "        \n",
    "        # Drop rows with NaN values in 'latitude' and 'longitude' columns\n",
    "        df.dropna(subset=['latitude', 'longitude'], inplace=True)\n",
    "        \n",
    "        # Save the updated DataFrame back to the CSV file with latitude and longitude columns\n",
    "        df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file to store the NaN counts. This was used for understanding the dataset.\n",
    "# Since the data set has been cleaned, it's no longer needed. Leaving it as a reference of our work.\n",
    "output_file = 'nan_counts.txt'\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    # Loop through the CSV files\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            # Read the CSV file into a DataFrame\n",
    "            filepath = os.path.join(input_dir, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "\n",
    "            # Count NaNs for each column in the DataFrame\n",
    "            nan_counts = df.isna().sum()\n",
    "\n",
    "            # Get the total number of rows\n",
    "            total_rows = len(df)\n",
    "\n",
    "            # Write the NaN counts and total rows for each file to the text file\n",
    "            f.write(f\"NaN counts and total rows for {filename}:\\n\")\n",
    "            for column in df.columns:\n",
    "                f.write(f\"{column}: {nan_counts[column]}\\n\")\n",
    "            f.write(f\"Total Rows: {total_rows}\\n\")\n",
    "            f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
